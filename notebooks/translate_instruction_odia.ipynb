{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYQdaXkfWVxi"
      },
      "source": [
        "## This script translate the instruction set by splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIEKn-5zWji3"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/gururise/AlpacaDataCleaned/main/alpaca_data_cleaned.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuFHKXkWWrWE"
      },
      "outputs": [],
      "source": [
        "# clone the repo for running evaluation\n",
        "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
        "%cd indicTrans\n",
        "# clone requirements repositories\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL4RRYPRWwPq"
      },
      "outputs": [],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "! pip install mosestokenizer subword-nmt\n",
        "# Install fairseq from source\n",
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "%cd fairseq\n",
        "# !git checkout da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d\n",
        "!pip install ./\n",
        "! pip install xformers\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY-3d2pFW2Bh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDu4JbEFW7qC"
      },
      "outputs": [],
      "source": [
        "# add fairseq folder to python path\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "# sanity check to see if fairseq is installed\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idy0ska2XDTQ"
      },
      "outputs": [],
      "source": [
        "# download the indictrans model\n",
        "\n",
        "\n",
        "# downloading the indic-en model\n",
        "# !wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/indic2en.zip\n",
        "# !unzip indic2en.zip\n",
        "\n",
        "# downloading the en-indic model\n",
        "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\n",
        "!unzip en2indic.zip\n",
        "\n",
        "# # downloading the indic-indic model\n",
        "# !wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.zip\n",
        "# !unzip m2m.zip\n",
        "\n",
        "%cd indicTrans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AY9EYc7JXHWp"
      },
      "outputs": [],
      "source": [
        "from indicTrans.inference.engine import Model\n",
        "\n",
        "indic2en_model = Model(expdir='../en-indic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vq32KzHfXLOf"
      },
      "outputs": [],
      "source": [
        "ta_sents = ['Happy birthday! May this special day bring you joy, laughter, and all the blessings you deserve. Have a wonderful year ahead!',\n",
        "            'During the last member meeting, create a list of 5 ideas to improve the club.',\n",
        "            'Design an algorithm to find the maximum profit of a stock buy and sell?']\n",
        "\n",
        "\n",
        "indic2en_model.batch_translate(ta_sents, 'en', 'or')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA7DNN08XPwP"
      },
      "outputs": [],
      "source": [
        "\n",
        "ta_paragraph = \"\"\"The U.S. economy more than doubled in size between 1989 and 2019, largely due to the rise of the information technology sector, improvements in worker productivity and consumer demand, and government investments. This growth was faster than in the prior two decades.\"\"\"\n",
        "\n",
        "indic2en_model.translate_paragraph(ta_paragraph, 'en', 'or')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnnK9NmnsTUP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUgYgAQ9s9U3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJk39DLisTnv"
      },
      "outputs": [],
      "source": [
        "# Recommeding to store the data in mounted google drive\n",
        "# !mkdir /content/drive/MyDrive/tranlation_work/translated_data /content/drive/MyDrive/tranlation_work/translated_data/data /content/drive/MyDrive/tranlation_work/translated_data/error "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FLZ83hqs-Nx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "translated_files = set(os.listdir('/content/drive/MyDrive/tranlation_work/translated_data/data'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wqj6iujKXTAg"
      },
      "outputs": [],
      "source": [
        "#Translate in chunks (e.g. 1K/5K)\n",
        "# import openai\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "# gdrive_path = '/content/drive/MyDrive/OdiaGen/Data/translated_instruction_data/'\n",
        "\n",
        "# Replace 'your_api_key' with your actual API key\n",
        "# openai.api_key = 'YOUR_API_KEY_COMES_HERE'\n",
        "\n",
        "def translate_text(value):\n",
        "    if \"\\n\" in value:\n",
        "      replace_dn = value.replace(\"\\n\\n\", \"\\n\")\n",
        "      split_lines = replace_dn.splitlines()\n",
        "      odia_splits = indic2en_model.batch_translate(split_lines, 'en', 'or')\n",
        "      response = \"\\n\".join(odia_splits)\n",
        "    else:\n",
        "      response = indic2en_model.translate_paragraph(value, 'en', 'or')\n",
        "    return response.strip()\n",
        "\n",
        "def translate_item(item):\n",
        "    translated_item = {}\n",
        "    for key, value in item.items():\n",
        "        if value:\n",
        "            translated_value = translate_text(value)\n",
        "            translated_item[key] = translated_value\n",
        "            translated_item['english_' + key] = value\n",
        "        else:\n",
        "            translated_item[key] = ''\n",
        "            translated_item['english_' + key] = ''\n",
        "    return translated_item\n",
        "\n",
        "\n",
        "def save_item(item, file_name):\n",
        "  with open(file_name, 'w') as f:\n",
        "    json.dump(item, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "def translate_save(item, i):\n",
        "    if f\"translated_{i}.json\" in translated_files:\n",
        "      return\n",
        "    if os.path.isfile( f\"/content/drive/MyDrive/tranlation_work/translated_data/data/translated_{i}.json\" ):\n",
        "      return\n",
        "    try:\n",
        "      translated_item = translate_item(item)\n",
        "      save_item(translated_item, f\"/content/drive/MyDrive/tranlation_work/translated_data/data/translated_{i}.json\")\n",
        "      print(f\"translated_{i}.json\", \"<---- succes\")\n",
        "    except Exception as e:\n",
        "      print(f\"translated_{i}.json: {e}\")\n",
        "      with open(f\"/content/drive/MyDrive/tranlation_work/translated_data/error/translated_{i}.json\", 'a'):\n",
        "          pass\n",
        "\n",
        "# Maximum number of parallel requests\n",
        "MAX_PARALLEL_REQUESTS = 50\n",
        "\n",
        "# Assuming the input JSON is in a file named 'input.json'\n",
        "with open('/content/alpaca_data_cleaned.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "    print(len(data))\n",
        "\n",
        "    \n",
        "\n",
        "start = 0\n",
        "#end = 52000\n",
        "# steps = 100\n",
        "end = 10000\n",
        "translated_data = []\n",
        "data = data[start:end]\n",
        "\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_PARALLEL_REQUESTS) as executor:\n",
        "    futures = {executor.submit(translate_save, item, i) for i, item in enumerate(data)}\n",
        "    \n",
        "    # for future in tqdm(as_completed(futures), total=len(futures), desc=\"Translating\"):\n",
        "    #     translated_data.append(future.result())\n",
        "\n",
        "# Save the translated data to a new JSON file named 'translated_data.json'\n",
        "#with open(f'translated_data_up_to_{start}_to_{end}.json', 'w') as f:\n",
        "#    json.dump(translated_data, f, ensure_ascii=False, indent=4)\n",
        "# with open(f'{gdrive_path}translated_data_up_to_{start}_to_{end}.json', 'w') as file:\n",
        "#   json.dump(translated_data, file, ensure_ascii=False, indent=4)\n",
        "# !cat '{gdrive_path}translated_data_up_to_{start}_to_{end}.json'\n",
        "\n",
        "print(f\"Translation complete. The translated data is saved in 'translated_data_up_to -> {start} _to_ {end}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX41VzgxvIBa"
      },
      "outputs": [],
      "source": [
        "def merge_json_files(data_folder):\n",
        "    merged_data = []\n",
        "    for i in range(50):\n",
        "        print(i)\n",
        "        \n",
        "        file_path = os.path.join(data_folder, f\"translated_{i}.json\")\n",
        "\n",
        "        if not os.path.isfile( file_path ):\n",
        "          continue\n",
        "        \n",
        "        with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
        "            data = json.load(file)\n",
        "            merged_data.append(data)\n",
        "    return merged_data\n",
        "\n",
        "def write_merged_json_file(output_file, merged_data):\n",
        "    with open(output_file, 'w', encoding=\"utf-8\") as file:\n",
        "        json.dump(merged_data, file, indent=2, ensure_ascii=False)\n",
        "\n",
        "data_folder = '/content/drive/MyDrive/tranlation_work/translated_data/data'\n",
        "output_file = '/content/drive/MyDrive/tranlation_work/odia_alpaca_data.json'\n",
        "\n",
        "merged_data = merge_json_files(data_folder)\n",
        "write_merged_json_file(output_file, merged_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaEKYJxIXYYs"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#files.download('translated_data_up_to_0_to_1.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sytAxBfhuNv"
      },
      "outputs": [],
      "source": [
        "#!pwd"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}